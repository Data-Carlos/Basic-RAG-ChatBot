{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial: Building a Knowledge-Enhanced Chatbot with LangChain and Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources\n",
    "Before you start, ensure you have the following:\n",
    "\n",
    "Python 3.7 or higher installed on your system.\n",
    "An OpenAI API key.\n",
    "A Pinecone API key.\n",
    "The dataset of Arvix papers (chunks_articles.csv).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the required libraries using pip:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install -qU langchain openai pinecone-client tiktoken langchain-community langsmith typing_extensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction and Setup\n",
    "To begin, we'll create a simple chatbot without any retrieval augmentation by initializing a ChatOpenAI object. Ensure you have your OpenAI API key ready.\n",
    "\n",
    "2. Loading and Preparing the Dataset\n",
    "Load the dataset of Arvix papers from a local CSV file (see file above) using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>chunk</th>\n",
       "      <th>id</th>\n",
       "      <th>submitter</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>journal-ref</th>\n",
       "      <th>doi</th>\n",
       "      <th>report-no</th>\n",
       "      <th>categories</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>versions</th>\n",
       "      <th>update_date</th>\n",
       "      <th>authors_parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401.08396_0</td>\n",
       "      <td>Hidden Flaws Behind Expert-Level Accuracy of  ...</td>\n",
       "      <td>2401.08396</td>\n",
       "      <td>Qiao Jin</td>\n",
       "      <td>Qiao Jin, Fangyuan Chen, Yiliang Zhou, Ziyang ...</td>\n",
       "      <td>Hidden Flaws Behind Expert-Level Accuracy of G...</td>\n",
       "      <td>Under review</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CV cs.AI cs.CL</td>\n",
       "      <td>http://creativecommons.org/licenses/by/4.0/</td>\n",
       "      <td>Recent studies indicate that Generative Pre-...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Tue, 16 Jan 202...</td>\n",
       "      <td>2024-04-24</td>\n",
       "      <td>[['Jin', 'Qiao', ''], ['Chen', 'Fangyuan', '']...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401.08396_1</td>\n",
       "      <td>g such multimodal AI models into clinical work...</td>\n",
       "      <td>2401.08396</td>\n",
       "      <td>Qiao Jin</td>\n",
       "      <td>Qiao Jin, Fangyuan Chen, Yiliang Zhou, Ziyang ...</td>\n",
       "      <td>Hidden Flaws Behind Expert-Level Accuracy of G...</td>\n",
       "      <td>Under review</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CV cs.AI cs.CL</td>\n",
       "      <td>http://creativecommons.org/licenses/by/4.0/</td>\n",
       "      <td>Recent studies indicate that Generative Pre-...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Tue, 16 Jan 202...</td>\n",
       "      <td>2024-04-24</td>\n",
       "      <td>[['Jin', 'Qiao', ''], ['Chen', 'Fangyuan', '']...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401.08396_2</td>\n",
       "      <td>es the correct final choices (35.5%), most pro...</td>\n",
       "      <td>2401.08396</td>\n",
       "      <td>Qiao Jin</td>\n",
       "      <td>Qiao Jin, Fangyuan Chen, Yiliang Zhou, Ziyang ...</td>\n",
       "      <td>Hidden Flaws Behind Expert-Level Accuracy of G...</td>\n",
       "      <td>Under review</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CV cs.AI cs.CL</td>\n",
       "      <td>http://creativecommons.org/licenses/by/4.0/</td>\n",
       "      <td>Recent studies indicate that Generative Pre-...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Tue, 16 Jan 202...</td>\n",
       "      <td>2024-04-24</td>\n",
       "      <td>[['Jin', 'Qiao', ''], ['Chen', 'Fangyuan', '']...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401.08396_3</td>\n",
       "      <td>Bethesda, MD, USA.  8Department of Neurology,...</td>\n",
       "      <td>2401.08396</td>\n",
       "      <td>Qiao Jin</td>\n",
       "      <td>Qiao Jin, Fangyuan Chen, Yiliang Zhou, Ziyang ...</td>\n",
       "      <td>Hidden Flaws Behind Expert-Level Accuracy of G...</td>\n",
       "      <td>Under review</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CV cs.AI cs.CL</td>\n",
       "      <td>http://creativecommons.org/licenses/by/4.0/</td>\n",
       "      <td>Recent studies indicate that Generative Pre-...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Tue, 16 Jan 202...</td>\n",
       "      <td>2024-04-24</td>\n",
       "      <td>[['Jin', 'Qiao', ''], ['Chen', 'Fangyuan', '']...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401.08396_4</td>\n",
       "      <td>an Peng, Ph.D., FAMIA Assistant Professor Depa...</td>\n",
       "      <td>2401.08396</td>\n",
       "      <td>Qiao Jin</td>\n",
       "      <td>Qiao Jin, Fangyuan Chen, Yiliang Zhou, Ziyang ...</td>\n",
       "      <td>Hidden Flaws Behind Expert-Level Accuracy of G...</td>\n",
       "      <td>Under review</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cs.CV cs.AI cs.CL</td>\n",
       "      <td>http://creativecommons.org/licenses/by/4.0/</td>\n",
       "      <td>Recent studies indicate that Generative Pre-...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Tue, 16 Jan 202...</td>\n",
       "      <td>2024-04-24</td>\n",
       "      <td>[['Jin', 'Qiao', ''], ['Chen', 'Fangyuan', '']...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       chunk_id                                              chunk  \\\n",
       "0  2401.08396_0  Hidden Flaws Behind Expert-Level Accuracy of  ...   \n",
       "1  2401.08396_1  g such multimodal AI models into clinical work...   \n",
       "2  2401.08396_2  es the correct final choices (35.5%), most pro...   \n",
       "3  2401.08396_3   Bethesda, MD, USA.  8Department of Neurology,...   \n",
       "4  2401.08396_4  an Peng, Ph.D., FAMIA Assistant Professor Depa...   \n",
       "\n",
       "           id submitter                                            authors  \\\n",
       "0  2401.08396  Qiao Jin  Qiao Jin, Fangyuan Chen, Yiliang Zhou, Ziyang ...   \n",
       "1  2401.08396  Qiao Jin  Qiao Jin, Fangyuan Chen, Yiliang Zhou, Ziyang ...   \n",
       "2  2401.08396  Qiao Jin  Qiao Jin, Fangyuan Chen, Yiliang Zhou, Ziyang ...   \n",
       "3  2401.08396  Qiao Jin  Qiao Jin, Fangyuan Chen, Yiliang Zhou, Ziyang ...   \n",
       "4  2401.08396  Qiao Jin  Qiao Jin, Fangyuan Chen, Yiliang Zhou, Ziyang ...   \n",
       "\n",
       "                                               title      comments  \\\n",
       "0  Hidden Flaws Behind Expert-Level Accuracy of G...  Under review   \n",
       "1  Hidden Flaws Behind Expert-Level Accuracy of G...  Under review   \n",
       "2  Hidden Flaws Behind Expert-Level Accuracy of G...  Under review   \n",
       "3  Hidden Flaws Behind Expert-Level Accuracy of G...  Under review   \n",
       "4  Hidden Flaws Behind Expert-Level Accuracy of G...  Under review   \n",
       "\n",
       "   journal-ref  doi  report-no         categories  \\\n",
       "0          NaN  NaN        NaN  cs.CV cs.AI cs.CL   \n",
       "1          NaN  NaN        NaN  cs.CV cs.AI cs.CL   \n",
       "2          NaN  NaN        NaN  cs.CV cs.AI cs.CL   \n",
       "3          NaN  NaN        NaN  cs.CV cs.AI cs.CL   \n",
       "4          NaN  NaN        NaN  cs.CV cs.AI cs.CL   \n",
       "\n",
       "                                       license  \\\n",
       "0  http://creativecommons.org/licenses/by/4.0/   \n",
       "1  http://creativecommons.org/licenses/by/4.0/   \n",
       "2  http://creativecommons.org/licenses/by/4.0/   \n",
       "3  http://creativecommons.org/licenses/by/4.0/   \n",
       "4  http://creativecommons.org/licenses/by/4.0/   \n",
       "\n",
       "                                            abstract  \\\n",
       "0    Recent studies indicate that Generative Pre-...   \n",
       "1    Recent studies indicate that Generative Pre-...   \n",
       "2    Recent studies indicate that Generative Pre-...   \n",
       "3    Recent studies indicate that Generative Pre-...   \n",
       "4    Recent studies indicate that Generative Pre-...   \n",
       "\n",
       "                                            versions update_date  \\\n",
       "0  [{'version': 'v1', 'created': 'Tue, 16 Jan 202...  2024-04-24   \n",
       "1  [{'version': 'v1', 'created': 'Tue, 16 Jan 202...  2024-04-24   \n",
       "2  [{'version': 'v1', 'created': 'Tue, 16 Jan 202...  2024-04-24   \n",
       "3  [{'version': 'v1', 'created': 'Tue, 16 Jan 202...  2024-04-24   \n",
       "4  [{'version': 'v1', 'created': 'Tue, 16 Jan 202...  2024-04-24   \n",
       "\n",
       "                                      authors_parsed  \n",
       "0  [['Jin', 'Qiao', ''], ['Chen', 'Fangyuan', '']...  \n",
       "1  [['Jin', 'Qiao', ''], ['Chen', 'Fangyuan', '']...  \n",
       "2  [['Jin', 'Qiao', ''], ['Chen', 'Fangyuan', '']...  \n",
       "3  [['Jin', 'Qiao', ''], ['Chen', 'Fangyuan', '']...  \n",
       "4  [['Jin', 'Qiao', ''], ['Chen', 'Fangyuan', '']...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"chunks_metadata.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Building the Knowledge Base\n",
    "Initializing Pinecone\n",
    "Set up your Pinecone API key and initialize the Pinecone client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "import os\n",
    "\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\") or \"\"\n",
    "\n",
    "# configure client\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up the Index Specification\n",
    "Configure the cloud provider and region for your index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "spec = ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the Index\n",
    "Create and initialize the index if it doesn't already exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 4860}},\n",
       " 'total_vector_count': 4860}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "index_name = 'rag'\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "# check if index already exists\n",
    "if index_name not in existing_indexes:\n",
    "    # create index\n",
    "    pc.create_index(index_name, dimension=1536, metric='dotproduct', spec=spec)\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Creating Embeddings and Populating the Index\n",
    "Instantiating the Embeddings Model\n",
    "Set up OpenAI's embedding model via LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "# Ensure you have your OpenAI API key set in your environment variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "# Instantiate the embeddings model\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding Text Data\n",
    "Generate embeddings for sample text data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1536)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = ['this is the first chunk of text', 'then another second chunk of text is here']\n",
    "\n",
    "res = embed_model.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding and Indexing the Dataset\n",
    "Embed and insert data into Pinecone in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:12<00:00,  3.03s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "data = dataset  # this makes it easier to iterate over the dataset\n",
    "batch_size = 30\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i + batch_size)\n",
    "    batch = data.iloc[i:i_end]\n",
    "    ids = [f\"{x['id']}-{x['chunk_id']}\" for _, x in batch.iterrows()]\n",
    "    texts = [str(x['chunk']) for _, x in batch.iterrows()]\n",
    "    embeds = embed_model.embed_documents(texts)\n",
    "    metadata = [{'text': str(x['chunk']), 'source': str(x['authors']), 'title': str(x['title'])} for _, x in batch.iterrows()]\n",
    "    index.upsert(vectors=list(zip(ids, embeds, metadata)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Retrieval Augmented Generation (RAG)\n",
    "Initializing the Vector Store\n",
    "Set up LangChain's vectorstore with our Pinecone index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\langchain_community\\vectorstores\\pinecone.py:68: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"  # the metadata field that contains our text\n",
    "\n",
    "# initialize the vector store object\n",
    "vectorstore = Pinecone(index, embed_model.embed_query, text_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying the Index\n",
    "Perform a similarity search to retrieve relevant information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Hidden Flaws Behind Expert-Level Accuracy of  Multimodal GPT-4 Vision in Medicine  Qiao Jin, M.D.1, Fangyuan Chen2, Yiliang Zhou, M.S.3, Ziyang Xu, M.D., Ph.D.4, Justin M. Cheung, M.D.5, Robert Chen, M.D.6, Ronald M. Summers, M.D., Ph.D.7, Justin F. Rousseau, M.D., M.M.Sc.8, Peiyun Ni, M.D.9, Marc J Landsman, M.D.10, Sally L. Baxter, M.D., M.Sc.11, Subhi J. Al'Aref, M.D.12, Yijia Li, M.D.13, Alex Chen14, M.D., Josef A. Brejt14, M.D., Michael F. Chiang, M.D15, Yifan Peng, Ph.D.3,* and Zhiyong Lu, Ph.D.1,*  Brief Abstract (70 words) We conducted a comprehensive evaluation of GPT-4V’s rationales when solving NEJM Image Challenges. We show that GPT-4V achieves comparable results to physicians regarding multi-choice accuracy (81.6% vs. 77.8%). However, we discovered that GPT-4V frequently presents flawed rationales in cases where it makes the correct final choices (35.5%), mostly in image comprehension. As such, our findings emphasize the necessity for in-depth evaluations before integratin\", metadata={'source': \"Qiao Jin, Fangyuan Chen, Yiliang Zhou, Ziyang Xu, Justin M. Cheung,\\n  Robert Chen, Ronald M. Summers, Justin F. Rousseau, Peiyun Ni, Marc J\\n  Landsman, Sally L. Baxter, Subhi J. Al'Aref, Yijia Li, Alex Chen, Josef A.\\n  Brejt, Michael F. Chiang, Yifan Peng, Zhiyong Lu\", 'title': 'Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine'}),\n",
       " Document(page_content='ests8,9. Recently, OpenAI released GPT-4 with Vision (GPT-4V), a state-of-the-art multimodal LLM that allows users to analyze both images and texts together. Subsequent pilot studies have been conducted to analyze the performance of GPT-4V in the medical domain10-13 (summarized in Supplementary Table 4). These evaluations mainly focused on the accuracy of GPT-4V in answering multi-choice medical questions, and in some cases, GPT-4V outperformed medical students and even physicians in closed-book settings. However, the multi-choice accuracy might not reflect the actual competence of GPT-4V, and there is no guarantee that correct final choices are based on accurate underlying rationales. Therefore, a thorough analysis is imperative to assess whether the decision-making of GPT-4V is based on sound rationales, rather than arbitrary conjecture.  To bridge this gap, we used over 200 multiple-choice questions with single correct answers from New England Journal of Medicine (NEJM) Image Challe', metadata={'source': \"Qiao Jin, Fangyuan Chen, Yiliang Zhou, Ziyang Xu, Justin M. Cheung,\\n  Robert Chen, Ronald M. Summers, Justin F. Rousseau, Peiyun Ni, Marc J\\n  Landsman, Sally L. Baxter, Subhi J. Al'Aref, Yijia Li, Alex Chen, Josef A.\\n  Brejt, Michael F. Chiang, Yifan Peng, Zhiyong Lu\", 'title': 'Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine'}),\n",
       " Document(page_content='blishing a student baseline. We then used a specifically designed prompt to ask GPT-4V to generate rationales in separate sections, which facilitates easier localization of the involved capability (described in Online Methods).  Fig. 1: Evaluation Procedure for GPT-4 with Vision (GPT-4V). This figure illustrates the evaluation workflow for GPT-4V using 207 NEJM Image Challenges. a, A medical student answered all questions and triaged them into specialties. b, Nine physicians provided their answers to the questions in their specialty. c, GPT-4V is prompted to answer challenge questions with a final choice and structured responses reflecting three specific capabilities. d, The physicians then appraised the validity of each component of GPT-4V’s responses based on the ground-truth explanations. \\n GPT-4V responses were manually recorded in independent chat sessions. Each question in the dataset was then categorized into a medical specialty and was annotated by one clinician in that field. ', metadata={'source': \"Qiao Jin, Fangyuan Chen, Yiliang Zhou, Ziyang Xu, Justin M. Cheung,\\n  Robert Chen, Ronald M. Summers, Justin F. Rousseau, Peiyun Ni, Marc J\\n  Landsman, Sally L. Baxter, Subhi J. Al'Aref, Yijia Li, Alex Chen, Josef A.\\n  Brejt, Michael F. Chiang, Yifan Peng, Zhiyong Lu\", 'title': 'Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine'})]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is GPT-4 Vision ?\"\n",
    "vectorstore.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmenting the Prompt\n",
    "Create a function to augment the chatbot's prompt with retrieved information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below, answer the query.\n",
      "\n",
      "    Contexts:\n",
      "    Hidden Flaws Behind Expert-Level Accuracy of  Multimodal GPT-4 Vision in Medicine  Qiao Jin, M.D.1, Fangyuan Chen2, Yiliang Zhou, M.S.3, Ziyang Xu, M.D., Ph.D.4, Justin M. Cheung, M.D.5, Robert Chen, M.D.6, Ronald M. Summers, M.D., Ph.D.7, Justin F. Rousseau, M.D., M.M.Sc.8, Peiyun Ni, M.D.9, Marc J Landsman, M.D.10, Sally L. Baxter, M.D., M.Sc.11, Subhi J. Al'Aref, M.D.12, Yijia Li, M.D.13, Alex Chen14, M.D., Josef A. Brejt14, M.D., Michael F. Chiang, M.D15, Yifan Peng, Ph.D.3,* and Zhiyong Lu, Ph.D.1,*  Brief Abstract (70 words) We conducted a comprehensive evaluation of GPT-4V’s rationales when solving NEJM Image Challenges. We show that GPT-4V achieves comparable results to physicians regarding multi-choice accuracy (81.6% vs. 77.8%). However, we discovered that GPT-4V frequently presents flawed rationales in cases where it makes the correct final choices (35.5%), mostly in image comprehension. As such, our findings emphasize the necessity for in-depth evaluations before integratin\n",
      "ests8,9. Recently, OpenAI released GPT-4 with Vision (GPT-4V), a state-of-the-art multimodal LLM that allows users to analyze both images and texts together. Subsequent pilot studies have been conducted to analyze the performance of GPT-4V in the medical domain10-13 (summarized in Supplementary Table 4). These evaluations mainly focused on the accuracy of GPT-4V in answering multi-choice medical questions, and in some cases, GPT-4V outperformed medical students and even physicians in closed-book settings. However, the multi-choice accuracy might not reflect the actual competence of GPT-4V, and there is no guarantee that correct final choices are based on accurate underlying rationales. Therefore, a thorough analysis is imperative to assess whether the decision-making of GPT-4V is based on sound rationales, rather than arbitrary conjecture.  To bridge this gap, we used over 200 multiple-choice questions with single correct answers from New England Journal of Medicine (NEJM) Image Challe\n",
      "blishing a student baseline. We then used a specifically designed prompt to ask GPT-4V to generate rationales in separate sections, which facilitates easier localization of the involved capability (described in Online Methods).  Fig. 1: Evaluation Procedure for GPT-4 with Vision (GPT-4V). This figure illustrates the evaluation workflow for GPT-4V using 207 NEJM Image Challenges. a, A medical student answered all questions and triaged them into specialties. b, Nine physicians provided their answers to the questions in their specialty. c, GPT-4V is prompted to answer challenge questions with a final choice and structured responses reflecting three specific capabilities. d, The physicians then appraised the validity of each component of GPT-4V’s responses based on the ground-truth explanations. \n",
      " GPT-4V responses were manually recorded in independent chat sessions. Each question in the dataset was then categorized into a medical specialty and was annotated by one clinician in that field. \n",
      "\n",
      "    Query: What is GPT-4 Vision ?\n"
     ]
    }
   ],
   "source": [
    "def augment_prompt(query: str):\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    return augmented_prompt\n",
    "\n",
    "# Example usage\n",
    "print(augment_prompt(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Let's try a query whitout RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my last update, there is no information available about a specific model called \"GPT-4 Vision.\" It is possible that newer models have been developed since then that I am not aware of. If you can provide more context or details, I can try to help you understand or provide information based on the latest available knowledge.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an expert in the field of AI.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like to understand Recursice Neural Networks.\")\n",
    "]\n",
    "chat = ChatOpenAI(openai_api_key=os.environ[\"OPENAI_API_KEY\"], model='gpt-3.5-turbo')\n",
    "\n",
    "# Asking the same question with no RAG\n",
    "prompt = HumanMessage(content=\"What is GPT-4 Vision?\")\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Integrating with the Chatbot\n",
    "Connect the augmented prompt to the chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4 Vision, also known as GPT-4V, is a state-of-the-art multimodal Large Language Model (LLM) developed by OpenAI. It allows users to analyze both images and texts together, enabling applications in various domains, including medicine. GPT-4V has been evaluated for its performance in answering multi-choice medical questions, where it has shown to outperform medical students and even physicians in closed-book settings.\n"
     ]
    }
   ],
   "source": [
    "# create a new user prompt and let's try with RAG this time \n",
    "prompt = HumanMessage(content=augment_prompt(query))\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "By following this tutorial, you've learned how to build a knowledge-enhanced chatbot that leverages a robust knowledge base using LangChain and Pinecone. This setup allows the chatbot to provide more accurate and contextually relevant responses by retrieving and integrating information from the knowledge base.\n",
    "\n",
    "Experiment with different datasets, queries, and embeddings to further enhance your chatbot's capabilities. Happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
